{"pages":[{"title":"archives","text":"","link":"/archives/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"about","text":"","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"数据库面试题-事务-1","text":"数据库事务四要素 数据库事务基本要素(ACID,原子性,一致性,隔离性,持久性) 原子性就是一个事务要么全部做完,要么全部不做。 一致性就是事务开始前和结束后并不破坏数据库的完整性约束。 隔离性就是同一时间,只允许一个事务请求同一数据,事务之间没有任何干扰。 持久性就是事务完成之后，事务对数据库的更新都会被保存到数据库,不能回滚。","link":"/2019/01/03/database/transaction/数据库面试题-事务-1/"},{"title":"排列组合","text":"温习一下排列组合的相关知识点","link":"/2019/01/09/probability/theory/PermutationAndCombination/"},{"title":"机器学习专有名词","text":"机器学习专有名词，概率论与数理统计的相关专有名词与符号说明 欧几里德空间: 简称欧式空间 实值函数 非负实值函数 统计规律举例 抛一枚均匀硬币出现正反面的可能性是一样的 样本空间:随机实验所有可能结果构成的集合，\\(S=\\{e\\}\\)，e称为样本点 概率论里面的事件 必然事件：把样本空间整体看做一个事件，则这个事件必然会发生，所以称为必然事件 随机事件A：样本空间的子集A为随机事件A 不可能事件\\(\\phi\\)：事件里面不包含任何样本点 基本事件：事件里面只包含一个样本点 PPL:衡量语言模型收敛情况 normalize:归一化 大数定律:https://blog.csdn.net/qizhuchuanghongdeng/article/details/69214822 泛函 符号表 符号 意义 \\(\\mathbf{X}\\) 输入空间 \\(\\mathbf{Y}\\) 输出空间 \\(\\mathbf{R}^n\\) n维实数向量空间，n维欧式空间 \\(\\mathbf{F}\\) 假设空间 \\(\\mathbf{R}_{exp}\\) 期望损失或者风险函数 \\(T=\\{(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)\\}\\) 训练数据集","link":"/2019/01/06/ML/ProperNoun/ProperNoun001/"},{"title":"贝叶斯分类算法","text":"贝叶斯分类算法是统计学的一种分类算法 贝叶斯分类算法 贝叶斯分类算法是统计学的一种分类算法 贝叶斯定理 贝叶斯推理等,是关于随机事件A和B的条件概率的一则定理, 其中P(A|B)是在B发生的情况下A发生的可能性 贝叶斯定理解决问的问题为 假设\\(H_1,H_2…,H_n\\)互斥且构成一个完全事件, 已知它们的概率\\(P(H_i),i=1,2,…,n,\\)现观察到某事件\\(A\\)与\\(H_1,H_2…,H_n\\)相伴随机出现, 且已知条件概率\\(P(A|H_i)\\),求\\(P(H_i|A)\\)。","link":"/2019/01/04/statistics/theory/BayesianClassificationAlgorithm001/"},{"title":"概率论基本概念","text":"概率论的基本概念，专有名词的定义等 事件间的关系与事件的运算 若\\(A \\subset B \\)，则称事件B包含事件A，事件A发生必导致事件B发生 相等：\\(若A \\subset B且B \\subset A,即A=B\\),则称事件A与事件B相等 和事件：\\(A \\bigcup B = \\{x|x \\in A 或 x \\in B\\}\\)称为事件A与事件B的和事件，当且仅当\\(A,B\\)中至少有一个发生时，事件\\(A \\bigcup B\\)发生 积事件：\\(A \\bigcap B = \\{x|x \\in A 且 x \\in B\\}\\)称为事件A与事件B的积事件，当且仅当\\(A,B\\)同时发生时，事件\\(A \\bigcap B\\)发生 差事件：\\(A - B=\\{x|x \\in A 且 x\\notin B\\}\\)称为事件A与事件B的差事件，当且仅当\\(A\\)发生，\\(B\\)不发生的时候，事件\\(A - B\\)发生 互不相容的事件/互斥的事件：若\\(A \\bigcap B = \\emptyset \\)，则称事件\\(A\\)与事件\\(B\\)是互不相容的，或者互斥的，这指的是事件\\(A\\)与事件\\(B\\)不可能同时发生。 基本事件是两两互斥的，两两互不相容的 逆事件/对立事件：若\\(A \\bigcup B = S 且 A \\bigcap B = \\emptyset \\)，则称事件A与事件B为互为逆事件，或者对立事件。 每次随机试验来看，要么事件\\(A\\)发生，要么事件\\(B\\)发生 随机事件\\(A\\)的对立事件记作\\(\\bar{A}\\)，\\(\\bar{A} = S - A \\) 事件运算遵循交换率，结合律，分配率，德摩根律 交换率：$$ \\begin{align}A \\bigcup B = B \\bigcup A \\\\A \\bigcap B = B \\bigcap A\\end{align} $$ 结合律：$$ \\begin{align}A \\bigcup (B \\bigcup C) = (A \\bigcup B) \\bigcup C \\\\A \\bigcap (B \\bigcap C) = (A \\bigcap B) \\bigcap C\\end{align} $$ 分配率：$$ \\begin{align}A \\bigcup (B \\bigcap C) = (A \\bigcup B) \\bigcap (A \\bigcup C) \\\\A \\bigcap (B \\bigcup C) = (A \\bigcap B) \\bigcup (A \\bigcap C)\\end{align} $$ 德•摩根定律：$$ \\begin{align}\\overline{A \\bigcup B} = \\overline{A} \\bigcap \\overline{B} \\\\\\overline{A \\bigcap B} = \\overline{A} \\bigcap \\overline{B}\\end{align} $$ 频率与概率 定义：在相同条件下，进行了\\(n\\)次试验，在这\\(n\\)次试验中，事件\\(A\\)发生的次数\\(n_A\\)称为事件\\(A\\)发生的频数，比值\\(n_A/n\\)称为事件A发生的频率，并记作\\(f_n(A)\\) 频率有如下性质 \\(0 \\leq f_n(A) \\leq 1\\) \\(f_n(S) = 1\\) 若\\(A_1,A_2,\\dots,A_k\\)是两两互不相容事件，则$$f_n(A_1 \\bigcup A_2 \\bigcup \\dots A_k )=f_n(A_1) + f_n(A_2) + \\dots + f_n(A_k)$$ 概率：设\\(E\\)为随机事件，\\(S\\)是样本空间，对于\\(E\\)的每一个事件\\(A\\)赋予一个实数，记作\\(P(A)\\)，称为事件\\(A\\)的概率，如果集合函数\\(P)\\满足下列条件 非负性：对于每个事件\\(A\\)，有\\(P(A) \\geq 0\\) 规范性：对于必然事件\\(S\\)，有\\(P(S) = 1\\) 可列可加性：设\\(A_1,A_2,\\dots\\)是两两互不相容的事件，即对于\\(A_iA_j=\\emptyset,i \\neq j,i,j=1,2,\\dots\\)有\\(P(A_1 \\bigcup A_2 \\bigcup) = P(A_i) + P(A_2) + \\dots\\) 频率与概率的关系：当\\(n \\rightarrow \\infty\\)时，频率\\(f_n(A)\\)在一定意义下接近于概率\\(P(A)\\)，基于这一事实，我们可以将概率\\(P(A)\\)用来表征 事件\\(A\\) 在一次试验中发生的可能性大小。 概率的一些重要的性质 \\(P(\\emptyset) = 0\\) 有限可加性$$P(A_1 \\bigcup A_2 \\bigcup \\dots A_n )=P(A_1) + P(A_2) + \\dots + P(A_n)$$ 设\\(A,B\\)是两个事件，若\\(A \\subset B\\)，则有$$\\begin{align}P(B-A)&amp;=P(B)-P(A) \\\\P(B) &amp; \\geq P(A)\\end{align} $$ 对于任一事件\\(A,P(A) \\leq 1\\) 逆事件的概率，对于任一事件\\(A\\)，有$$P(\\overline{A})=1-P(A)$$ 加法公式，对于任意事件\\(A,B\\)，有$$P(A \\bigcup B) = P(A) + P(B) + P(AB)$$ 推广到三个事件\\(A,B,C\\)，则有$$P(A \\bigcup B \\bigcup C) = P(A)+P(B)+P(C)-P(AB)-P(BC)-P(AC)+P(ABC)$$ 等可能概型(古典概型) 随机试验的样本空间具有有限个样本点，每个基本事件发生的可能性相同，这类随机试验称为等可能模型(古典概型) 等可能概型中，随机事件\\(A\\)的概率计算为$$P(A)=\\sum_{j=1}^{k}P(\\{e_{ij}\\})=\\frac{k}{n}=\\frac{A包含的基本事件数}{S中包含的基本事件数}$$ 条件概率独立性定义 确定性现象：一定条件下一定会发生的现象，比如向上抛石子必然会下落 统计规律性：大量重复试验或者观察中所呈现的固有规律性 随机现象：一类在个别试验中其结果呈现出不确定性，在大量重复试验中其结果又是具有统计规律性的现象叫做随机现象 随机试验具有以下三个特点： 可以在相同条件下重复进行 每次试验的可能结果不止一个，并且能事先明确试验的所有可能结构 进行一次试验之前不能确定哪一个结果会出现 样本空间：随机试验\\(E\\)的所有可能结果组成的集合为样本空间，记作\\(S\\) 样本点：样本空间的元素称为样本点 基本事件：由一个样本点组成的单点集合，我们成为基本事件 随机事件：样本空间\\(S\\)的子集为随机事件，每次随机试验中，当且仅当这一子集中的一个样本点出现时，我们称为这一事件发生 必然事件：样本空间\\(S\\)本身称为必然事件 不可能事件：空集\\(\\emptyset\\)不包含任何样本点，每次随机试验必不会发生，所以\\(\\emptyset\\)称为不可能事件","link":"/2019/01/08/probability/theory/introduction/"},{"title":"语言模型的评估","text":"一个模型做出来之后，总要评估模型的效果，语言模型也一样，本文会介绍语言模型的评估方法之一：计算困惑度/迷惑度/混乱度 迷惑度/困惑度/混乱度(Perplexity)注 参考资料1:语言模型评价指标Perplexity 参考资料2:困惑度详解（perplexity） 参考资料3:NLP点滴——文本相似度","link":"/2019/01/07/ML/application/LanguageModel/evaluate/"},{"title":"监督学习-统计学习三要素","text":"统计学习方法的不同主要来自其模型，策略，算法的不同。确定了模型，策略，算法，统计学习的方法也就确定了。这就是将模型，策略，算法称之为统计学习三要素的原因。 统计学习三要素$$统计学习方法=模型+策略+算法$$ 模型 在监督学习中，模型就是所要学习的条件概率分布或决策函数。 模型的假设空间包含所有可能的条件概率分布或决策函数。 假设空间中的模型一般有无穷多个 假设空间可以定义为决策函数的集合:$$\\mathbf{F}=\\{f|Y=f(X)\\}$$\\(X\\)与\\(Y\\)是定义在输入空间\\(X\\)和输出空间\\(Y\\)的随机变量，这时\\(F\\)通常是由一个参数向量决定的函数族:$$\\mathbf{F}=\\{f|Y=f_{\\theta}(X),\\theta\\in\\mathbf{R}^n\\}$$参数向量\\(\\theta\\)取值于\\(n\\)维度欧式空间\\(\\mathbf{R}^n\\)，称为参数空间 假设空间也可以定义为条件概率的集合:$$\\mathbf{F}=\\{P|P(Y|X)\\}$$\\(X\\)与\\(Y\\)是定义在输入空间\\(X\\)和输出空间\\(Y\\)的随机变量，这时\\(F\\)通常是由一个参数向量决定的条件概率分布族:$$\\mathbf{F}=\\{P|P_{\\theta}(Y|X),\\theta\\in\\mathbf{R}^n\\}$$参数向量\\(\\theta\\)取值于\\(n\\)维度欧式空间\\(\\mathbf{R}^n\\)，也称为参数空间 通常称由决策函数表示的模型为非概率模型，条件概率表示的模型为概率模型 策略 有了模型的假设空间之后，接下来考虑按照什么样的准则学习或者选择最优的模型。 损失函数(loss function)也叫代价函数(cost function)：损失函数/代价函数度量模型一次预测的好坏，损失函数是\\(f(X)\\)和\\(Y\\)的非负实值函数，记作\\(L(Y,f(X))\\)，损失函数值越小，模型就越好 统计学习常用的损失函数有以下几种: 0-1损失函数(0-1 loss function)$$L(Y,f(X))=\\begin{cases}1, &amp; Y \\ne f(X) \\\\0, &amp; Y = f(X)\\end{cases}$$ 平方损失函数$$L(Y,f(X))=(Y-f(X))^2$$ 绝对损失函数$$L(Y,f(X))=|Y-f(X)|$$ 对数损失函数/对数似然损失函数$$L(Y,P(Y|X))=-\\log{P(Y|X)}$$ 风险函数(risk function)或者期望损失(Expected loss)：风险函数度量平均意义下模型预测的好坏，也就是损失函数的期望$$\\mathbf{R}_{exp}(f)=E_p[L(Y,f(X))]=\\int_{x*y}L(y,f(x))P(x,y)dxdy$$由于并不知道\\(P(X,Y)\\)，所以\\(R_{exp}\\)并不能直接进行计算 经验风险(empirical risk)或者经验损失(empirical loss): 给定一个训练数据集$$T=\\{(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)\\}$$模型\\(f(X)\\)关于训练数据集的平均损失称为经验损失，记作$$R_{emp}=\\frac{1}{N}\\sum_{i=1}^{N}L(y_i,f(x_i))$$ 期望风险与经验风险的区别为 期望风险\\(R_{exp}(f)\\)是模型关于联合分布的期望损失 经验风险\\(R_{emp}(f)\\)是模型关于训练数据集的期望损失 根据大数定律，当样本容量\\(N\\)趋于无穷的时候，经验风险\\(R_{emp}(f)\\)趋于期望风险\\(R_{exp}(f)\\)，但是现实情况下，使用经验风险估计期望风险常常并不理想，所以要对经验风险进行矫正，这关系到两个策略:经验风险最小化和结构风险最小化 经验风险最小化(empirical risk minimization ERM) 在假设空间，损失函数，训练数据集确定的情况下，经验风险函数式是可以确定的，经验风险最小化(ERM)认为经验风险最小化的模型为最优模型。$$\\mathop{\\min}\\limits_{f\\in\\mathbf{F}}\\frac{1}{N}\\sum_{i=1}^{N}L(y_i,f(x_i))$$\\(\\mathbf{F}\\)为假设空间 经验风险最小化在样本容量足够大的时候，有比较好的学习效果，比如: 极大似然估计(maximum likelihood estimation)，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计 经验风险最小化在样本容量很小的时候，学习效果未必好，会产生”过拟合(over-fitting)”现象 结构风险最小化(structural risk minimization SRM) 结构风险最小化是为了防止过拟合而提出来的策略 结构风险最小化等同于正则化(regularization) 结构风险在经验风险上加上模型复杂度的正则化项或罚项，在假设空间，损失函数，以及训练数据集确定的情况下，结构风险的定义为$$R_{srm}(f)=\\frac{1}{N}\\sum_{i=1}{N}L(y_i,f(x_i))+\\lambda J(f)$$\\(J(f)\\)为模型的复杂度，是定义在假设空间的泛函 模型\\(f\\)越复杂，复杂度\\(J(f)\\)越大 模型\\(f\\)越简单，复杂度\\(J(f)\\)越小 \\(\\lambda \\geq 0\\)是系数，用来权衡经验风险和模型复杂度。 结构风险小需要经验风险与模型复杂度同时小 结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测 结构风险最小化例子 贝叶斯估计中的最大后验概率估计就是结构风险最小化的一个例子，当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计 结构风险最小化的策略认为结构风险最小的模型是最优的模型，所以最优的模型的求解为$$\\mathop{\\min}\\limits_{f\\in\\mathbf{F}}\\frac{1}{N}\\sum_{i=1}^{N}L(y_i,f(x_i))+\\lambda J(f)$$ 算法 主要是指学习模型的具体计算方法，统计学习模型可以利用已有的最优化算法，有时也需要开发独自的最优化算法。 注: 本文多参考自《统计学习方法》，李航著，偏于个人学习笔记的整理。","link":"/2019/01/08/ML/statisticsML/theory/SupervisedLearning002/"},{"title":"监督学习概论","text":"监督学习(supervised learning)的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测。监督学习是及其重要的统计学习分支，也是统计学习中内容最丰富，应用最广泛的部分，本文主要讲述 监督学习的基本概念 输入空间，特征空间，输出空间 输入空间：输入所有可能的取值的集合 特征空间：每个具体的输入是一个实例，通常由特征向量(feature vector)表示，这时，所有特征向量存在的空间称之为特征空间(feature space) 特征空间的每一纬对应与一个特征 输出空间：输出所有可能的取值的集合 输入空间与输出空间的关系 输入与输出空间可以是有限元素的集合，也可以是整个欧式空间 输入与输出空间可以是同一空间，也可以是不同的空间 通常输出空间远远小于输入空间 输入空间与特征空间 有时假设输入空间与特征空间为相同空间 有时假设输入空间与特征空间为不同的空间，将实例(输入)映射到特征空间。 模型实际上都是定义在特征空间上的 输入实例\\(x\\)的特征向量记作$$x=(x^{(1)},x^{(2)},\\dots,x^{(i)},\\dots,x^{(n)})^T$$ 训练数据由输入(或特征向量)与输出对组成，训练集通常表示为$$\\mathbf{T}={(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)}$$ 测试数据也由输入(或特征向量)与输出对组成，又称之为样本或者样本点 输入变量\\(X\\)和输出变量\\(Y\\)可以有不同的类型(连续或者是离散的)，人们习惯根据输入变量和输出变量的类型定义以下几种预测任务类型 回归问题：输入变量和输出变量均为连续变量的预测问题 分类问题：输出变量为有限个离散变量的预测任务 标注问题：输入变量与输出变量均为变量序列的预测问题 联合概率分布 监督学习假设输入与输出的随机变量\\(X\\)和\\(Y\\)遵循联合概率分布\\(P(X,Y)\\)，\\(P(X,Y)\\)表示分布函数，或者分布密度函数。 在学习过程中，假定这一联合概率分布存在，但对学习系统来说，联合概率分布的具体定义是未知的。 训练集数据与测试集数据被看做是依联合改了分布\\(P(X,Y)\\)独立同分布产生的。 统计学习假设数据存在一定的统计规律，\\(X\\)和\\(Y\\)遵循联合概率分布\\(P(X,Y)\\)就是监督学习关于数据的基本假设 假设空间 假设空间是指输入空间到输出空间的映射的集合。 假设空间的设定代表学习范围的确定。 问题的形式化 在学习过程中，学习系统利用给定的训练集数据，通过学习，训练得到一个模型，表示为条件概率分布\\(\\widehat{P}(Y|X)\\)或决策函数\\(Y=\\widehat{f}(X)\\) 条件概率分布\\(\\widehat{P}(Y|X)\\)或决策函数\\(Y=\\widehat{f}(X)\\)描述输入与输出随机变量之间的映射关系 注: 本文多参考自《统计学习方法》，李航著，偏于个人学习笔记的整理。","link":"/2019/01/06/ML/statisticsML/theory/SupervisedLearning001/"},{"title":"统计学习方法概论","text":"统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科，统计学习也称为统计机器学习。(引自&lt;&lt;统计学习方法&gt;&gt;) 统计学习方法概论统计学习(statistics learning) 统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科，统计学习也称为统计机器学习。(引自&lt;&lt;统计学习方法&gt;&gt;) 统计学习的特点 统计学习的特点主要有以下几点 统计学习以数据为研究对象，是数据驱动的学科 统计学习目的是对数据进行预测与分析 统计学习是以方法为中心，统计学习方法构建模型，并且应用模型对数据进行预测与分析 统计学习是概率论，统计学，信息论，最优化理论及计算机学科等多个领域的交叉学科，并且在发展中逐步行程肚子的理论体系与方法论统计学习的对象 统计学习的对象是数据(data)，从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到数据的预测与分析中。 统计学习关于数据的基本假设是同类数据具有一定的统计规律性，这是统计学习的前提 因为统计学习研究的数据都具有统计规律性，所以可以使用概率统计方法加以处理 e.g. 可以用随机变量描述数据中的特征，用概率分布描述数据的统计规律 统计学习的过程中，以变量或变量组表示数据 数据分为连续变量表示的类型和离散变量表示的类型统计学习的目的:对数据进行预测与分析统计学习的方法: 分类 监督学习(supervised learning) 非监督学习(unsupervised learning) 半监督学习(semi-supervised learning) 强化学习(reinforcement learning) 统计学习方法的三要素 模型(model) 策略(strategy) 算法(algorithm) 实现统计学习方法的步骤 得到一个有限的训练数据集合 确定包含所有可能的模型的假设空间，即学习模型的集合 确定模型选择的准则，即学习的策略 实现求解最优模型的算法，即学习的算法 通过学习方法选择最优模型 利用学习的最优模型对新数据进行预测或分析 注: 本文多参考自《统计学习方法》，李航著，偏于个人学习笔记的整理。","link":"/2019/01/05/ML/statisticsML/theory/introduction/"},{"title":"Gensim入门乱七八糟","text":"初次接触Gensim，这里主要是一些随笔记录 123from gensim.models import word2vecsentences = word2vec.LineSentence(\"已经分好词的，用户语句文件\")model = word2vec.Word2Vec(sentences, size=100, hs=1, min_count=1, window=3)","link":"/2019/01/07/Coding/Python/Package/Gensim/essay/"}],"tags":[{"name":"数据库事务","slug":"数据库事务","link":"/tags/数据库事务/"},{"name":"面试","slug":"面试","link":"/tags/面试/"},{"name":"概率论","slug":"概率论","link":"/tags/概率论/"},{"name":"排列组合","slug":"排列组合","link":"/tags/排列组合/"},{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"机器学习专有名词","slug":"机器学习专有名词","link":"/tags/机器学习专有名词/"},{"name":"统计学","slug":"统计学","link":"/tags/统计学/"},{"name":"分类算法","slug":"分类算法","link":"/tags/分类算法/"},{"name":"贝叶斯分类算法","slug":"贝叶斯分类算法","link":"/tags/贝叶斯分类算法/"},{"name":"机器学习应用","slug":"机器学习应用","link":"/tags/机器学习应用/"},{"name":"语言模型","slug":"语言模型","link":"/tags/语言模型/"},{"name":"语言模型评估","slug":"语言模型评估","link":"/tags/语言模型评估/"},{"name":"统计机器学习","slug":"统计机器学习","link":"/tags/统计机器学习/"},{"name":"监督学习","slug":"监督学习","link":"/tags/监督学习/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Gensim","slug":"Gensim","link":"/tags/Gensim/"}],"categories":[{"name":"数据库","slug":"数据库","link":"/categories/数据库/"},{"name":"概率论","slug":"概率论","link":"/categories/概率论/"},{"name":"机器学习","slug":"机器学习","link":"/categories/机器学习/"},{"name":"统计学","slug":"统计学","link":"/categories/统计学/"},{"name":"事务","slug":"数据库/事务","link":"/categories/数据库/事务/"},{"name":"基本概念","slug":"概率论/基本概念","link":"/categories/概率论/基本概念/"},{"name":"专有名词","slug":"机器学习/专有名词","link":"/categories/机器学习/专有名词/"},{"name":"分类算法","slug":"统计学/分类算法","link":"/categories/统计学/分类算法/"},{"name":"语言模型","slug":"语言模型","link":"/categories/语言模型/"},{"name":"统计机器学习","slug":"机器学习/统计机器学习","link":"/categories/机器学习/统计机器学习/"},{"name":"评估","slug":"语言模型/评估","link":"/categories/语言模型/评估/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Gensim","slug":"Python/Gensim","link":"/categories/Python/Gensim/"}]}